package gopstaticcrawler

import (
	"os"
	"path/filepath"
	"time"

	"github.com/hophouse/gop/utils"
	"github.com/hophouse/gop/utils/logger"
)

func RunCrawlerCmd() {
	begin := time.Now()

	// bars
	progressBars := utils.InitWaitGroupBar()

	// Init the crawler
	c := InitCrawler()
	utils.CrawlerBar = progressBars.AddBar("Crawler", true)

	if *GoCrawlerOptions.ScreenshotPtr {
		utils.ScreenshotBar = progressBars.AddBar("Screenshot", false)
	}

	logger.Printf("\n")
	logger.Printf("[+] Crawling from URL: %s\n\n", Yellow(*GoCrawlerOptions.UrlPtr))

	// Start the crawler
	VisiteURL(&URLVisited, c, *GoCrawlerOptions.UrlPtr)

	progressBars.Wait()

	// If report option
	if *GoCrawlerOptions.ReportPtr {
		WriteRessourceListReport(append(External_ressources, Internal_ressources...))
	}

	// If the screenshot option is enabled
	// if *GoCrawlerOptions.ScreenshotPtr {
	// }

	// Stop time
	end := time.Now()

	// Save results in a file
	internalRessourcesFile, _ := os.Create(filepath.Join(logger.CurrentLogDirectory, "internal_urls.txt"))
	for _, item := range Internal_ressources {
		_, err := internalRessourcesFile.WriteString(item.Url + "\n")
		if err != nil {
			logger.Printf("Error writing : %s\n", item.Url)
		}
	}
	internalRessourcesFile.Close()

	externalRessourcesFile, _ := os.Create(filepath.Join(logger.CurrentLogDirectory, "external_urls.txt"))
	for _, item := range External_ressources {
		_, err := externalRessourcesFile.WriteString(item.Url + "\n")
		if err != nil {
			logger.Printf("Error writing : %s\n", item.Url)
		}
	}
	externalRessourcesFile.Close()

	// Print Statistics
	PrintRessourcesResume("Internal", *GoCrawlerOptions.UrlPtr, Internal_ressources)
	PrintRessourcesResume("External", *GoCrawlerOptions.UrlPtr, External_ressources)

	PrintStatistics(end.Sub(begin), Internal_ressources, External_ressources)
}
