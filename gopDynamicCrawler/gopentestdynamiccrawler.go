package gopdynamiccrawler

import (
	"os"
	"path/filepath"
	"time"

	gopstaticcrawler "github.com/hophouse/gop/gopStaticCrawler"
	"github.com/hophouse/gop/gopchromedp"
	"github.com/hophouse/gop/utils"
	"github.com/hophouse/gop/utils/logger"
)

// RunCrawlerCmd Run the crawler
func RunCrawlerCmd() {
	begin := time.Now()

	// bars
	progressBars := utils.InitWaitGroupBar()

	// Init the crawler
	InitCrawler()

	utils.CrawlerBar = progressBars.AddBar("Crawler", true)

	logger.Printf("\n")
	logger.Printf("[+] Crawling from URL: %s\n\n", gopstaticcrawler.Yellow(*GoCrawlerOptions.UrlPtr))

	// Launch the workers
	for i := 0; i < *GoCrawlerOptions.ConcurrencyPtr; i++ {
		go workerVisit()
	}

	// Add first URL
	utils.CrawlerBar.AddAndIncrementTotal(1)
	UrlChan <- *GoCrawlerOptions.UrlPtr

	// Wait for all workers to finish
	progressBars.Wait()
	defer close(UrlChan)

	// If the screenshot option is enabled
	if *GoCrawlerOptions.ScreenshotPtr == true {

		progressBars := utils.InitWaitGroupBar()
		utils.ScreenshotBar = progressBars.AddBar("Screenshot", false)

		screenShotList := []gopchromedp.Item{}

		utils.ScreenshotBar.AddAndIncrementTotal(len(Internal_ressources))

		// Create channel
		inputChan := make(chan string)
		workerChan := make(chan bool, *GoCrawlerOptions.ConcurrencyPtr)
		resultChan := make(chan gopchromedp.Item)

		// Run workers
		for i := 0; i < *GoCrawlerOptions.ConcurrencyPtr; i++ {
			go func(inputChan chan string, workerChan chan bool, resultChan chan gopchromedp.Item) {
				for requestURL := range inputChan {
					logger.Fprintf(logger.Writer(), "[+] Taking screenshot for %s\n", requestURL)
					item := gopchromedp.NewItem(requestURL)

					// Go make the screenshot
					gopchromedp.TakeScreenShot(&item, filepath.Join(logger.CurrentLogDirectory, "screenshots"), *GoCrawlerOptions.ProxyPtr, *GoCrawlerOptions.CookiePtr, *GoCrawlerOptions.DelayPtr)

					// Add screenshot to list
					resultChan <- item
				}
				workerChan <- true
			}(inputChan, workerChan, resultChan)
		}

		// GoRoutine to consume the results
		go func(workerChan chan bool, resultChan chan gopchromedp.Item, screenShotList *[]gopchromedp.Item) {
			for screenshot := range resultChan {
				*screenShotList = append(*screenShotList, screenshot)
			}
			workerChan <- true
		}(workerChan, resultChan, &screenShotList)

		// Add url to the input list
		for _, request := range Internal_ressources {
			inputChan <- request.Url
		}
		close(inputChan)

		// Wait for the workers to finish their jobs
		for i := 0; i < *GoCrawlerOptions.ConcurrencyPtr; i++ {
			<-workerChan
		}
		close(resultChan)

		// Wait for results to be treated
		<-workerChan
		close(workerChan)

		// Create HTML index page
		f, err := os.Create(filepath.Join(logger.CurrentLogDirectory, "./index.html"))
		if err != nil {
			logger.Println(err)
		}
		defer f.Close()
		f.WriteString(gopchromedp.ExportHTMLPage())

		// Export loaded resources
		f, err = os.Create(filepath.Join(logger.CurrentLogDirectory, "./loaded_resources.txt"))
		if err != nil {
			logger.Println(err)
		}
		defer f.Close()
		f.WriteString(gopchromedp.ExportLoadedResources(screenShotList))

		// Export data as JSON
		itemsJSON := gopchromedp.ExportItemsToJSON(screenShotList)
		f, err = os.Create(filepath.Join(logger.CurrentLogDirectory, "./json/data.json"))
		if err != nil {
			logger.Println(err)
		}
		defer f.Close()
		f.WriteString(itemsJSON)

		// Export data as JSON in JavaScript object
		f, err = os.Create(filepath.Join(logger.CurrentLogDirectory, "./json/data.json.js"))
		if err != nil {
			logger.Println(err)
		}
		defer f.Close()
		f.WriteString("const itemsJSON = " + itemsJSON)

		progressBars.Wait()
	}

	// If report option
	//if *GoCrawlerOptions.ReportPtr == true {
	//	gopstaticcrawler.WriteRessourceListReport(append(External_ressources, Internal_ressources...))
	//}

	// Stop time
	end := time.Now()

	// Save results in a file
	internalRessourcesFile, _ := os.Create(filepath.Join(logger.CurrentLogDirectory, "internal_urls.txt"))
	for _, item := range Internal_ressources {
		internalRessourcesFile.WriteString(item.Url + "\n")
	}
	internalRessourcesFile.Close()

	externalRessourcesFile, _ := os.Create(filepath.Join(logger.CurrentLogDirectory, "external_urls.txt"))
	for _, item := range External_ressources {
		externalRessourcesFile.WriteString(item.Url + "\n")
	}
	externalRessourcesFile.Close()

	// Print Statistics
	gopstaticcrawler.PrintRessourcesResume("Internal", *GoCrawlerOptions.UrlPtr, Internal_ressources)
	gopstaticcrawler.PrintRessourcesResume("External", *GoCrawlerOptions.UrlPtr, External_ressources)

	gopstaticcrawler.PrintStatistics(end.Sub(begin), Internal_ressources, External_ressources)
}
